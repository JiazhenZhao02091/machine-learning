### 导入数据并观察
# 忽视警告，这个库是内置的，不需要安装
import warnings
warnings.filterwarnings('ignore')
import pandas  as pd

data = pd.read_csv('data.csv')
# # 随机打乱数据集，这一步的操作是可选的，类似于把练习题不断更改顺序，防止电脑学习到固定的顺序，这一步是可选的
# random_seed = 77
# data = data.sample(frac=1, random_state=random_seed)
data.head()
# 从这里可以看出没有空缺值，所以不用处理缺失值
data.info()
# 删除无用特征 customerID
data.drop('customerID', axis=1, inplace=True)
data.head()
data.info()
data.shape
# 删除特征后，我们对特征进行编码，这里我们对离散特征进行独特编码，连续特征做标准化处理
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder # 用于特征标准化、独热编码和数值编码
from sklearn.model_selection import train_test_split # 用于将数据集划分为训练集和测试集

# 分离特征与标签
X = data.drop('Churn', axis=1)
y = data['Churn']

# 删除高基数字段
X = X.drop(columns=["TotalCharges"])  # 可以使用目标编码用统计值来代替高基数字段

X
## 编码
### 离散特征和连续特征编码
# 如果需要填补缺失值，使用众数
# for column in X.columns:
#     if X[column].dtype in ['float64', 'int64']:
#         mode = X[column].mode()[0]
#         X[column].fillna(mode, inplace=True)

# 分离离散特征和连续特征  这里我们把 int 和 float 都看作连续特征， 字符串看作离散特征
continuous_features = X.select_dtypes(include=['float64', 'int64']).columns
discrete_features = X.select_dtypes(include=['object']).columns


# 对连续特征进行标准化处理
scaler = StandardScaler()
X_continuous = scaler.fit_transform(X[continuous_features])
# 对离散特征进行独热编码  // 这里可以考虑使用标签编码减少特征的数量
encoder = OneHotEncoder()
X_discrete = encoder.fit_transform(X[discrete_features])
# 合并连续特征和离散特征
X = pd.concat([pd.DataFrame(X_continuous), pd.DataFrame(X_discrete.toarray())], axis=1)

# 对标签进行数值编码
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y
## 数据预处理
# 可以看出标签有点不平衡，所以后面我们需要进行过采样
y.dtype  # 此时不是series类型了
y = pd.Series(y)
y.value_counts()
**划份数据集**
# 使用 train_test_split 函数按照 8:2 的比例划分数据集
# test_size=0.2 表示 20%的数据用作测试集，即验证集。
# random_state 是一个随机数种子，确保每次划分的结果相同，便于复现结果。
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# X_test.columns = X.columns.astype(str)
### K-means 引入新的特征
#### 使用肘部法则判断K值的选取 --下降幅度明显趋向于缓慢的时候，取该值为K的值
import numpy as np  # 导入 NumPy 库，用于数值计算
from sklearn.cluster import KMeans  # 从 scikit-learn 库中导入 KMeans 聚类算法
from sklearn.manifold import TSNE  # 从 scikit-learn 库中导入 TSNE，用于降维
from sklearn.decomposition import PCA #导入pca
from scipy.spatial.distance import cdist  # 从 SciPy 库中导入 cdist 函数，用于计算距离
import matplotlib.pyplot as plt  # 导入 Matplotlib 库，用于数据可视化

# 设置 Matplotlib 字体以避免字体缺失的警告
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用黑体显示中文
plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号

# 使用肘部法则确定最佳的 K 值
K = range(1, 10)  # 选择 K 的范围
mean_distortions = []  # 存储每个 K 值对应的平均畸变程度

for k in K:  # 遍历每个 K 值
    k_means = KMeans(n_clusters=k)  # 初始化 KMeans 模型，设置聚类数为 k
    k_means.fit(X)  # 训练 KMeans 模型
    centers = k_means.cluster_centers_  # 获取聚类中心
    error = sum(np.min(cdist(X, centers, 'euclidean'), axis=1))  # 计算每个点到其最近聚类中心的距离，并求和
    mean_distortions.append(error)  # 将误差添加到列表中

# 绘制肘部法则图
plt.plot(K, mean_distortions, 'bx-')  # 绘制 K 值与平均畸变程度的关系图，使用蓝色的 'x' 标记点，并用线连接
plt.xlabel('k')  # 设置 x 轴标签为 'k'
plt.ylabel('平均畸变程度')  # 设置 y 轴标签为 '平均畸变程度'
plt.title('用肘部法则来确定最佳的 K 值')  # 设置图表标题为 '用肘部法则来确定最佳的 K 值'
# plt.savefig("iris1.png", bbox_inches='tight')  # 保存图表为 'iris1.png' 文件，bbox_inches='tight' 表示紧凑布局
plt.show()  # 显示图表
#### 使用K-means聚类分析引入新的特征，包括所属类别，距离质心的距离
# 这里的最佳K值从上面获得，找下降幅度最大的点，这里是3
from sklearn.cluster import KMeans  # 从 scikit-learn 库中导入 KMeans 聚类算法
from scipy.spatial.distance import cdist  # 从 SciPy 库中导入 cdist 函数，用于计算距离

# 训练模型
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
# 读取聚类标签
cluster_labels = kmeans.labels_
# 计算距离质心距离
distances_to_centroids = kmeans.transform(X)
# 获取每个样本到其所属质心的距离
distance_to_own_centroid = distances_to_centroids[range(len(X)), kmeans.labels_]

# 将特征添加至数据集中
X['Cluster'] = cluster_labels
X['Distance_to_Centroid'] = distance_to_own_centroid

# 打印看看
X.head()
#### 由于k-means对于离散变量并不友好，这里改用 k-prototype聚类
# 导入必要的库
from kmodes.kprototypes import KPrototypes
import numpy as np
import matplotlib.pyplot as plt

# 分离离散特征和连续特征
continuous_features = X.select_dtypes(include=['float64', 'int64']).columns
discrete_features = X.select_dtypes(include=['object']).columns
categorical_indices = [X.columns.get_loc(col) for col in discrete_features]

# 使用肘部法确定K-prototypes的最佳K值
cost = []
K = range(1, 10)

for k in K:
    if k == 1:
        cost.append(float('inf'))
        continue
        
    kproto = KPrototypes(n_clusters=k, random_state=42)
    clusters = kproto.fit_predict(X.values, categorical=categorical_indices)
    cost.append(kproto.cost_)

# 绘制肘部图
plt.figure(figsize=(10, 6))
plt.plot(K, cost, 'bo-')
plt.xlabel('K值 (簇的数量)')
plt.ylabel('成本函数值')
plt.title('K-prototypes的肘部图')
plt.grid(True)
plt.show()

# 找到肘部点
cost_diff = np.diff(cost)
cost_diff_rate = cost_diff[:-1] / cost_diff[1:]
elbow_point = np.argmax(cost_diff_rate) + 1
print(f"根据肘部法，最佳K值为: {K[elbow_point]}")

# 使用最佳K值训练最终模型
best_k = K[elbow_point]
kproto = KPrototypes(n_clusters=best_k, random_state=42)
clusters = kproto.fit_predict(X.values, categorical=categorical_indices)

# 计算每个样本到其所属质心的距离
distances = kproto.transform(X.values)
distance_to_own_centroid = distances[range(len(X)), clusters]

# 将特征添加至数据集中
X['Proto_Cluster'] = clusters
X['Distance_to_Proto_Centroid'] = distance_to_own_centroid

# 打印看看结果
print("\n聚类结果预览：")
print(X[['Proto_Cluster', 'Distance_to_Proto_Centroid']].head())
# 打印看看数据
X_train.info()
# 发现有重复列名 需要处理

# 方法1：为重复列名添加后缀
def rename_duplicates(df):
    cols = pd.Series(df.columns)
    for dup in cols[cols.duplicated()].unique(): 
        cols[cols[cols == dup].index.values.tolist()] = [f"{dup}_{i}" for i in range(sum(cols == dup))]
    df.columns = cols
    return df
X_train = rename_duplicates(X_train)
X_test = rename_duplicates(X_test)
### 离散特征和连续特征编码   注：实际运行时应该保证在进行k-prototype算法之前，离散变量不应该进行处理，这里省略了步骤
# SMOTE要求列名都是str
X_train.columns = X.columns.astype(str)
# 模型要求数据统一类型
X_test.columns = X.columns.astype(str)
### SMOTE过采样
# 对训练集数据进行过采样， 这里我们使用SMOTE插值法产生样本 不能对考试即测试集进行过采样
from imblearn.over_sampling import SMOTE # 导入 SMOTE 方法

# 使用 SMOTE 进行过采样,并赋值给自己
smote = SMOTE(random_state=42)  # 随机数种子我们还是设置为 42
X_train, y_train = smote.fit_resample(X_train, y_train)
X_train
## 模型训练

---
至此为止，数据初步处理已经完成，接下来训练模型
# 这里我们的基模型选择使用逻辑回归、随机森林、XGBoost、LightGBM 四种模型
# 首先不调参，看看效果
# 导入所需的库
import pandas as pd  # 用于数据处理和分析
import numpy as np  # 用于数值计算
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder  # 用于数据预处理
from sklearn.model_selection import train_test_split, GridSearchCV  # 用于数据集划分和超参数调优
from sklearn.linear_model import LinearRegression  # 线性回归模型
from sklearn.linear_model import LogisticRegression  # 逻辑回归模型
from sklearn.svm import SVC  # 支持向量机分类模型
from sklearn.naive_bayes import GaussianNB  # 高斯朴素贝叶斯模型
from sklearn.ensemble import RandomForestClassifier  # 随机森林分类模型
import xgboost as xgb  # XGBoost模型
import lightgbm as lgb  # LightGBM模型
from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, classification_report  # 用于模型评估
import warnings
# 忽略所有警告
warnings.filterwarnings("ignore")
import seaborn as sns
import matplotlib.pyplot as plt # 绘图
### 不调参情况
#### 逻辑回归
# 逻辑回归
print('逻辑回归')

lr = LogisticRegression()  # 实例化逻辑回归模型
lr.fit(X_train, y_train)  # 训练模型
y_pred_lr = lr.predict(X_test)  # 预测

# y_pred_lr
# 输出
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))
print("Classification Report:\n", classification_report(y_test, y_pred_lr))
print("AUC: ", roc_auc_score(y_test, y_pred_lr))
#### 随机森林
# 随机森林
print('随机森林')

# 定义随机森林模型（使用默认参数）
rf = RandomForestClassifier(random_state=42)

# 训练模型
rf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred_proba = rf.predict_proba(X_test)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print(" AUC: ", roc_auc_score(y_test, y_pred_proba))
# X_test.columns = X.columns.astype(str)
# print("原始列名类型:", X_test.columns.map(type).unique())
#### SVM
# SVM
print('SVM')

# 定义 SVM 模型（使用默认参数）
svm = SVC(probability=True, random_state=42)  # 启用概率估计

# 训练模型
svm.fit(X_train, y_train)

# 在测试集上进行预测
y_pred_proba = svm.predict_proba(X_test)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
# 数据转numpy 避免类型错误
X_train_np = X_train.values
y_train_np = y_train.values
X_test_np = X_test.values
y_test_np = y_test.values
#### XGBoost
# XGBoost
print('XGBoost')

# 定义 XGBoost 模型（使用默认参数）
xgb_model = xgb.XGBClassifier(random_state=42)

# 训练模型
xgb_model.fit(X_train_np, y_train_np)

# 在测试集上进行预测
y_pred_proba = xgb_model.predict_proba(X_test_np)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
#### LightGBM
# LightGBM
print('LightGBM')

# 定义 LightGBM 模型（使用默认参数）
lgb_model = lgb.LGBMClassifier(random_state=42)

# 训练模型
lgb_model.fit(X_train_np, y_train_np)

# 在测试集上进行预测
y_pred_proba = lgb_model.predict_proba(X_test_np)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
# y_pred_proba
### 调参，这里我们使用网格搜索（还可以使用optuna，启发式算法等）
#### 逻辑回归
# 逻辑回归

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score


# 定义逻辑回归模型
lr = LogisticRegression()

# 定义参数网格
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],  # 正则化强度的倒数
    'penalty': ['l1', 'l2'],       # 正则化类型
    'solver': ['liblinear']        # 优化算法（liblinear 支持 l1 和 l2）
}

# 初始化网格搜索
grid_search = GridSearchCV(
    estimator=lr,  # 模型
    param_grid=param_grid,  # 参数网格
    cv=5,  # 五折交叉验证
    scoring='roc_auc',  # 使用 AUC 作为评估指标
    n_jobs=-1  # 使用所有可用的CPU核心
)

# 在训练集上执行网格搜索
grid_search.fit(X_train, y_train)

# 输出最佳参数
print("最佳参数组合:", grid_search.best_params_)

# 使用最佳参数训练模型
best_lr = grid_search.best_estimator_  # best_lr 为最佳参数模型

# 在测试集上进行预测
y_pred_lr = best_lr.predict(X_test)

# # 输出混淆矩阵
# print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))

# # 输出分类报告
# print("Classification Report:\n", classification_report(y_test, y_pred_lr))

# 输出 AUC
y_pred_proba = best_lr.predict_proba(X_test)[:, 1]  # 获取正类的概率
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
#### SVM
# SVM
# 定义 SVM 模型
svm = SVC(probability=True, random_state=42)  # 启用概率估计

# 定义参数网格
param_grid = {
    'C': [0.1, 1, 10],  # 正则化参数
    'kernel': ['linear', 'rbf'],  # 核函数
    'gamma': ['scale', 'auto']  # 核函数的系数
}

# 初始化网格搜索
grid_search = GridSearchCV(
    estimator=svm,  # 模型
    param_grid=param_grid,  # 参数网格
    cv=5,  # 五折交叉验证
    scoring='roc_auc',  # 使用 AUC 作为评估指标
    n_jobs=-1  # 使用所有可用的CPU核心
)

# 在训练集上执行网格搜索
grid_search.fit(X_train, y_train)

# 输出最佳参数
print("最佳参数组合:", grid_search.best_params_)

# 使用最佳参数训练模型
best_svm = grid_search.best_estimator_

# 在测试集上进行预测
y_pred_svm = best_svm.predict_proba(X_test)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_svm))
#### 随机森林
# 随机森林
# 定义随机森林模型
rf = RandomForestClassifier(random_state=42)

# 定义参数网格
param_grid = {
    'n_estimators': [50, 100, 200],  # 树的数量
    'max_depth': [None, 10, 20],     # 每棵树的最大深度
    'min_samples_split': [2, 5, 10], # 分裂内部节点所需的最小样本数
    'min_samples_leaf': [1, 2, 4],   # 叶子节点所需的最小样本数
    'max_features': ['sqrt', 'log2'] # 每棵树分裂时考虑的最大特征数
}

# 初始化网格搜索
grid_search = GridSearchCV(
    estimator=rf,  # 模型
    param_grid=param_grid,  # 参数网格
    cv=5,  # 五折交叉验证
    scoring='roc_auc',  # 使用 AUC 作为评估指标
    n_jobs=-1  # 使用所有可用的CPU核心
)

# 在训练集上执行网格搜索
grid_search.fit(X_train_np, y_train_np)

# 输出最佳参数
print("最佳参数组合:", grid_search.best_params_)

# 使用最佳参数训练模型
best_rf = grid_search.best_estimator_

# 在测试集上进行预测
y_pred_proba = best_rf.predict_proba(X_test_np)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
#### XGBoost
# XGBoost
# 定义 XGBoost 模型
xgb_model = xgb.XGBClassifier(random_state=42)

# 定义参数网格
param_grid = {
    'max_depth': [3, 6, 9],  # 树的最大深度
    'learning_rate': [0.01, 0.1, 0.3],  # 学习率
    'n_estimators': [100, 200, 300],  # 树的数量
    'subsample': [0.8, 1.0],  # 样本采样比例
    'colsample_bytree': [0.8, 1.0]  # 特征采样比例
}

# 初始化网格搜索
grid_search = GridSearchCV(
    estimator=xgb_model,  # 模型
    param_grid=param_grid,  # 参数网格
    cv=5,  # 五折交叉验证
    scoring='roc_auc',  # 使用 AUC 作为评估指标
    n_jobs=-1  # 使用所有可用的CPU核心
)

# 在训练集上执行网格搜索
grid_search.fit(X_train_np, y_train_np)

# 输出最佳参数
print("最佳参数组合:", grid_search.best_params_)

# 使用最佳参数训练模型
best_xgb = grid_search.best_estimator_

# 在测试集上进行预测
y_pred_proba = best_xgb.predict_proba(X_test_np)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
#### LightGBM

# LightGBM
# 定义 LightGBM 模型
lgb_model = lgb.LGBMClassifier(random_state=42)

# 定义参数网格
param_grid = {
    'num_leaves': [31, 63, 127],  # 每棵树的最大叶子节点数
    'learning_rate': [0.01, 0.1, 0.3],  # 学习率
    'n_estimators': [100, 200, 300],  # 树的数量
    'max_depth': [5, 10, -1],  # 树的最大深度，-1 表示不限制
    'subsample': [0.8, 1.0],  # 样本采样比例
    'colsample_bytree': [0.8, 1.0]  # 特征采样比例
}

# 初始化网格搜索
grid_search = GridSearchCV(
    estimator=lgb_model,  # 模型
    param_grid=param_grid,  # 参数网格
    cv=5,  # 五折交叉验证
    scoring='roc_auc',  # 使用 AUC 作为评估指标
    n_jobs=-1  # 使用所有可用的CPU核心
)

# 在训练集上执行网格搜索
grid_search.fit(X_train_np, y_train_np)

# 输出最佳参数
print("最佳参数组合:", grid_search.best_params_)

# 使用最佳参数训练模型
best_lgb = grid_search.best_estimator_

# 在测试集上进行预测
y_pred_proba = best_lgb.predict_proba(X_test_np)[:, 1]  # 获取正类的概率

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
### 可视化所有模型AUC
# import matplotlib.pyplot as plt
# from sklearn.metrics import roc_curve, auc
# from sklearn.model_selection import cross_val_predict
# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
# from sklearn.naive_bayes import GaussianNB
# from sklearn.ensemble import RandomForestClassifier
# import xgboost as xgb
# import lightgbm as lgb

# # 假设你已经有了数据集 X_train, X_test, y_train, y_test

# # 定义模型及其最佳参数
# models = {
#     "Logistic Regression": LogisticRegression(),
#     "SVM": SVC(C=10, gamma='auto', kernel='rbf', probability=True),
#     "Naive Bayes": GaussianNB(var_smoothing=1e-5),
#     "Random Forest": RandomForestClassifier(
#         max_depth=10, max_features='sqrt', min_samples_leaf=1,
#         min_samples_split=10, n_estimators=200
#     ),
#     "XGBoost": xgb.XGBClassifier(
#         colsample_bytree=0.8, learning_rate=0.1, max_depth=3,
#         n_estimators=100, subsample=0.8
#     ),
#     "LightGBM": lgb.LGBMClassifier(
#         colsample_bytree=0.8, learning_rate=0.1, max_depth=5,
#         n_estimators=100, num_leaves=31, subsample=0.8
#     )
# }

# # 绘制验证集的 ROC 曲线
# plt.figure(figsize=(12, 6))
# plt.subplot(1, 2, 1)  # 左边图：验证集

# for name, model in models.items():
#     # 使用交叉验证获取验证集的预测概率
#     y_pred_val = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba')[:, 1]
#     fpr, tpr, _ = roc_curve(y_train, y_pred_val)
#     roc_auc = auc(fpr, tpr)
#     plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')

# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Validation Set ROC Curves')
# plt.legend(loc="lower right")

# # 绘制测试集的 ROC 曲线
# plt.subplot(1, 2, 2)  # 右边图：测试集

# for name, model in models.items():
#     # 训练模型并在测试集上进行预测
#     model.fit(X_train, y_train)
#     y_pred_test = model.predict_proba(X_test)[:, 1]
#     fpr, tpr, _ = roc_curve(y_test, y_pred_test)
#     roc_auc = auc(fpr, tpr)
#     plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')

# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Test Set ROC Curves')
# plt.legend(loc="lower right")

# plt.tight_layout()
# plt.show()
### 模型融合（这里直接使用调参后的模型，不考虑未调参的模型）
#### Stacking进行模型融合
**这里使用逻辑回归、随机森林、XGBoost作为基模型，LightGBM做元模型**
##### 回归问题

# from sklearn.ensemble import StackingClassifier  # 导入 Stacking 分类器
# from sklearn.linear_model import LogisticRegression  # 逻辑回归模型
# from sklearn.ensemble import RandomForestClassifier  # 随机森林分类模型
# import xgboost as xgb  # XGBoost模型
# import lightgbm as lgb  # LightGBM模型
# # 定义基模型（使用调参后的最优参数）
# base_models = [
#     ('rf', RandomForestClassifier(
#         max_depth=20,  # 最优参数
#         max_features=0.8,  # 最优参数
#         min_samples_leaf=1,  # 最优参数
#         min_samples_split=2,  # 最优参数
#         n_estimators=120,  # 最优参数
#         random_state=42
#     )),  # 随机森林
#     ('xgb', XGBClassifier(
#         colsample_bytree=1.0,  # 最优参数
#         learning_rate=0.1,  # 最优参数
#         max_depth=9,  # 最优参数
#         n_estimators=150,  # 最优参数
#         reg_alpha=0,  # 最优参数
#         reg_lambda=1,  # 最优参数
#         subsample=0.8,  # 最优参数
#         randsample_bytree=1.0,  # 最优参数
#         learning_rate=0.1,  # 最优参数
#         num_leaves=100,  # 最优参数
#         subsample=0.8,  # 最优参数
#         random_state=42
#     ))  # 逻辑回归
# ]
# # 定义元模型（LightGBM）
# meta_model_lgbm = LGBMClassifier(
#     colsample_bytree=0.9,  # 微调
#     learning_rate=0.05,  # 降低学习率
#     num_leaves=50,  # 微调
#     n_estimators=200,  # 增加树的数量
#     reg_alpha=0.1,  # 微调 L1 正则化
#     reg_lambda=0.5,  # 微调 L2 正则化om_state=42
#     )),  # XGBoost
#     ('lr', LogisticRegression(
#         col
#     subsample=0.9,  # 微调
#     random_state=42
# )

# # 创建 Stacking 回归器
# stacking_model_lgbm = StackingClassifier(
#     estimators=base_models,  # 基模型
#     final_estimator=meta_model_lgbm,  # LightGBM 作为元模型
#     n_jobs=1  # 使用1 CPU 核心
# )

# # 训练 Stacking 模型
# stacking_model_lgbm.fit(X_train, y_train)

# # 预测
# y_test_pred_lgbm = stacking_model_lgbm.predict(X_test)[:, 1]

# # 计算 AUC

# print("测试集 AUC: ", roc_auc_score(y_test, y_test_pred_lgbm))
##### 分类问题

# ... 已有导入 ...
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression  

# 修正后的基模型定义
base_models = [
    ('rf', RandomForestClassifier(
        max_depth=20,
        max_features=0.8,
        min_samples_leaf=1,
        min_samples_split=2,
        n_estimators=120,
        random_state=42
    )),
    ('xgb', XGBClassifier(
        colsample_bytree=1.0,
        learning_rate=0.1,
        max_depth=9,
        n_estimators=150,
        reg_alpha=0,
        reg_lambda=1,
        subsample=0.8,
        random_state=42,
        use_label_encoder=False,  # 分类任务需添加该参数
        eval_metric='logloss'      # XGBoost分类器推荐参数
    )),
    ('lr', LogisticRegression(
        C=1.0,  # 正则化强度的倒数
        max_iter=1000,  # 增加迭代次数
        random_state=42,
        solver='lbfgs'  # 适用于多分类的优化算法
    ))
]

# 修正后的元模型（LightGBM分类器）
meta_model = LGBMClassifier(
    colsample_bytree=0.9,
    learning_rate=0.05,
    num_leaves=50,
    n_estimators=200,
    reg_alpha=0.1,
    reg_lambda=0.5,
    subsample=0.9,
    random_state=42
)

# 创建Stacking分类器
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    stack_method='predict_proba',  # 分类任务使用概率预测
    n_jobs=1
)

# 训练和预测
stacking_model.fit(X_train, y_train)
y_pred_proba = stacking_model.predict_proba(X_test)[:, 1]  # 获取正类概率
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))
#### 加权进行融合

##### 回归
# # 定义基模型（使用你提供的参数）
# base_models = [
#     ('rf', RandomForestClassifier(
#         max_depth=20,  # 最优参数
#         max_features=0.8,  # 最优参数
#         min_samples_leaf=1,  # 最优参数
#         min_samples_split=2,  # 最优参数
#         n_estimators=120,  # 最优参数
#         random_state=42
#     )),  # 随机森林
#     ('xgb', XGBClassifier(
#         colsample_bytree=1.0,  # 最优参数
#         learning_rate=0.1,  # 最优参数
#         max_depth=9,  # 最优参数
#         n_estimators=150,  # 最优参数
#         reg_alpha=0,  # 最优参数
#         reg_lambda=1,  # 最优参数
#         subsample=0.8,  # 最优参数
#         random_state=42
#     )),  # XGBoost
#     ('lgbm', LGBMClassifier(
#         colsample_bytree=1.0,  # 最优参数
#         learning_rate=0.1,  # 最优参数
#         num_leaves=100,  # 最优参数
#         subsample=0.8,  # 最优参数
#         random_state=42
#     ))  # LightGBM
# ]

# # 训练基模型
# for name, model in base_models:
#     model.fit(X_train, y_train)
#     print(f"{name} 模型训练完成")

# # 获取每个模型的预测结果
# y_test_pred_rf = base_models[0][1].predict(X_test)  # 随机森林
# y_test_pred_xgb = base_models[1][1].predict(X_test)  # XGBoost
# y_test_pred_lgbm = base_models[2][1].predict(X_test)  # LightGBM

# # 定义网格搜索范围
# weights_rf = np.linspace(0, 1, 11)  # 随机森林权重范围 [0, 0.1, ..., 1]
# weights_xgb = np.linspace(0, 1, 11)  # XGBoost 权重范围 [0, 0.1, ..., 1]
# weights_lgbm = np.linspace(0, 1, 11)  # LightGBM 权重范围 [0, 0.1, ..., 1]

# # 初始化最佳 AUC 和最佳权重
# best_rmse = float('inf')
# best_weights = None

# # 网格搜索
# for w_rf in weights_rf:
#     for w_xgb in weights_xgb:
#         for w_lgbm in weights_lgbm:
#             if w_rf + w_xgb + w_lgbm == 1:  # 确保权重之和为 1
#                 # 加权平均
#                 y_test_pred_weighted = (w_rf * y_test_pred_rf +
#                                         w_xgb * y_test_pred_xgb +
#                                         w_lgbm * y_test_pred_lgbm)
#                 # 计算 RMSE
#                 test_rmse = mean_squared_error(y_test, y_test_pred_weighted, squared=False)
#                 # 更新最佳权重
#                 if test_rmse < best_rmse:
#                     best_rmse = test_rmse
#                     best_weights = (w_rf, w_xgb, w_lgbm)

# # 输出最佳权重和 RMSE
# print(f"最佳权重：随机森林={best_weights[0]:.2f}, XGBoost={best_weights[1]:.2f}, LightGBM={best_weights[2]:.2f}")
# print(f"最佳 AUC: {best_rmse:.4f}")
##### 分类
base_models = [
    ('rf', RandomForestClassifier(
        max_depth=20,
        max_features=0.8,
        min_samples_leaf=1,
        min_samples_split=2,
        n_estimators=120,
        random_state=42
    )),  # 随机森林
    ('xgb', XGBClassifier(
        colsample_bytree=1.0,
        learning_rate=0.1,
        max_depth=9,
        n_estimators=150,
        reg_alpha=0,
        reg_lambda=1,
        subsample=0.8,
        random_state=42
    )),  # XGBoost
    # 替换为逻辑回归
    ('lr', LogisticRegression(
        C=1.0,  # 正则化参数
        max_iter=1000,  # 增加迭代次数确保收敛
        random_state=42,
        solver='lbfgs'  # 适用于中小型数据集的优化器
    ))  # 逻辑回归
]
for name, model in base_models:
    model.fit(X_train, y_train)
    print(f"{name} 模型训练完成")

# 获取预测结果时修改变量名
y_test_pred_rf = base_models[0][1].predict(X_test)  # 随机森林
y_test_pred_xgb = base_models[1][1].predict(X_test)  # XGBoost
y_test_pred_lr = base_models[2][1].predict(X_test)   # 逻辑回归

# 修改网格搜索权重变量名
weights_rf = np.linspace(0, 1, 11)
weights_xgb = np.linspace(0, 1, 11)
weights_lr = np.linspace(0, 1, 11)  # 修改为逻辑回归权重

auc = -100

for w_rf in weights_rf:
    for w_xgb in weights_xgb:
        for w_lr in weights_lr:  # 修改循环变量
            if w_rf + w_xgb + w_lr == 1:  # 更新变量名
                y_test_pred_weighted = (w_rf * y_test_pred_rf +
                                        w_xgb * y_test_pred_xgb +
                                        w_lr * y_test_pred_lr)  # 修改变量名
                tmp_auc = roc_auc_score(y_test, y_test_pred_weighted)
                auc = max(tmp_auc, auc)

print(f"最佳 AUC: {auc}")
### 使用阈值进行二次调参 thresholds
from sklearn.metrics import accuracy_score, roc_auc_score

model = stacking_model

# 在测试集上进行预测
y_pred_proba = model.predict_proba(X_test)[:, 1]

# 输出测试集 AUC
print("测试集 AUC: ", roc_auc_score(y_test, y_pred_proba))

# 初始化列表用于存储不同阈值下的准确率 ---  这里使用准确率作为评价指标
accuracies = []
thresholds = np.arange(0.1, 1, 0.01)

for threshold in thresholds:
    y_pred = (y_pred_proba >= threshold).astype(int)
    accuracy = accuracy_score(y_test, y_pred)  # 之前使用 AUC 值，现在使用准确率
    accuracies.append(accuracy)

# 找到最大准确率及其对应的阈值
max_accuracy_index = np.argmax(accuracies)
optimal_threshold = thresholds[max_accuracy_index]
max_accuracy = accuracies[max_accuracy_index]

print(f"最佳阈值: {optimal_threshold}")
print(f"最大准确率: {max_accuracy}")

# 绘制阈值 - 准确率曲线
plt.plot(thresholds, accuracies)
plt.xlabel('Threshold')
plt.ylabel('Accuracy')
plt.title('Threshold - Accuracy Curve')
plt.show()
## 结果可视化
# import matplotlib.pyplot as plt
# from sklearn.metrics import roc_curve, auc
# from sklearn.model_selection import cross_val_predict
# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
# from sklearn.naive_bayes import GaussianNB
# from sklearn.ensemble import RandomForestClassifier
# import xgboost as xgb
# import lightgbm as lgb

# # 假设你已经有了数据集 X_train, X_test, y_train, y_test

# # 定义模型及其最佳参数
# models = {
#     "Logistic Regression": LogisticRegression(),
#     "SVM": SVC(C=10, gamma='auto', kernel='rbf', probability=True),
#     "Naive Bayes": GaussianNB(var_smoothing=1e-5),
#     "Random Forest": RandomForestClassifier(
#         max_depth=10, max_features='sqrt', min_samples_leaf=1,
#         min_samples_split=10, n_estimators=200
#     ),
#     "XGBoost": xgb.XGBClassifier(
#         colsample_bytree=0.8, learning_rate=0.1, max_depth=3,
#         n_estimators=100, subsample=0.8
#     ),
#     "LightGBM": lgb.LGBMClassifier(
#         colsample_bytree=0.8, learning_rate=0.1, max_depth=5,
#         n_estimators=100, num_leaves=31, subsample=0.8
#     )
# }

# # 绘制验证集的 ROC 曲线
# plt.figure(figsize=(12, 6))
# plt.subplot(1, 2, 1)  # 左边图：验证集

# for name, model in models.items():
#     # 使用交叉验证获取验证集的预测概率
#     y_pred_val = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba')[:, 1]
#     fpr, tpr, _ = roc_curve(y_train, y_pred_val)
#     roc_auc = auc(fpr, tpr)
#     plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')

# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Validation Set ROC Curves')
# plt.legend(loc="lower right")

# # 绘制测试集的 ROC 曲线
# plt.subplot(1, 2, 2)  # 右边图：测试集

# for name, model in models.items():
#     # 训练模型并在测试集上进行预测
#     model.fit(X_train, y_train)
#     y_pred_test = model.predict_proba(X_test)[:, 1]
#     fpr, tpr, _ = roc_curve(y_test, y_pred_test)
#     roc_auc = auc(fpr, tpr)
#     plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')

# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Test Set ROC Curves')
# plt.legend(loc="lower right")

# plt.tight_layout()
# plt.show()
## 可解释分析

---

可以使用Shap、pdpbox库进行可视化分析
# shap解释需要计算shap值，才可以后续可视化
import shap
import lightgbm as lgb

explain_model = stacking_model   # 设置训练好的模型
# 使用 shap 进行解释
# 创建一个 SHAP 解释器，使用训练好的 stacking_model 模型
explainer = shap.Explainer(stacking_model)
# 计算 SHAP 值，使用训练集
shap_values = explainer.shap_values(X_train)
# SHAP 汇总图默认设置为点图，可以反应特征的重要性和影响方向
shap.summary_plot(shap_values[0], X_train,plot_type="dot", max_display=15)
# SHAP 汇总图设置为条形图，可以显示特征重要性
shap.summary_plot(shap_values[1], X_train, plot_type="bar")
# 可视化所有类别的SHAP 汇总图
shap.summary_plot(shap_values, X_train)
### pdpbox
from pdpbox import pdp, info_plots

# 选择要分析的特征名称，替换 'feature_name' 为实际特征名
feature_name = 'hour'

# 计算部分依赖值
pdp_feature = pdp.pdp_isolate(
    model=stacking_model,
    dataset=X_train,
    model_features=X_train.columns,
    feature=feature_name
)

# 绘制部分依赖图
fig, axes = pdp.pdp_plot(pdp_feature, feature_name)

# 显示图形
import matplotlib.pyplot as plt
plt.show()
## 模型持久化
"""
from joblib import dump,load

# 保存模型
dump(stacking_model, 'model.joblib')

# 加载模型
model = load('model.joblib')
"""
