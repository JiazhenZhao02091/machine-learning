{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据并观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽视警告，这个库是内置的，不需要安装\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "# # 随机打乱数据集，这一步的操作是可选的，类似于把练习题不断更改顺序，防止电脑学习到固定的顺序，这一步是可选的\n",
    "# random_seed = 77\n",
    "# data = data.sample(frac=1, random_state=random_seed)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从这里可以看出没有空缺值，所以不用处理缺失值\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除无用特征 customerID\n",
    "data.drop('customerID', axis=1, inplace=True)\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除特征后，我们对特征进行编码，这里我们对离散特征进行独特编码，连续特征做标准化处理\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder # 用于特征标准化、独热编码和数值编码\n",
    "from sklearn.model_selection import train_test_split # 用于将数据集划分为训练集和测试集\n",
    "\n",
    "# 分离特征与标签\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# 删除高基数字段\n",
    "X = X.drop(columns=[\"TotalCharges\"])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means 引入新的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用肘部法则判断K值的选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 导入 NumPy 库，用于数值计算\n",
    "from sklearn.cluster import KMeans  # 从 scikit-learn 库中导入 KMeans 聚类算法\n",
    "from sklearn.manifold import TSNE  # 从 scikit-learn 库中导入 TSNE，用于降维\n",
    "from sklearn.decomposition import PCA #导入pca\n",
    "from scipy.spatial.distance import cdist  # 从 SciPy 库中导入 cdist 函数，用于计算距离\n",
    "import matplotlib.pyplot as plt  # 导入 Matplotlib 库，用于数据可视化\n",
    "\n",
    "# 设置 Matplotlib 字体以避免字体缺失的警告\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用黑体显示中文\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 正常显示负号\n",
    "\n",
    "# 使用肘部法则确定最佳的 K 值\n",
    "K = range(1, 10)  # 选择 K 的范围\n",
    "mean_distortions = []  # 存储每个 K 值对应的平均畸变程度\n",
    "\n",
    "for k in K:  # 遍历每个 K 值\n",
    "    k_means = KMeans(n_clusters=k)  # 初始化 KMeans 模型，设置聚类数为 k\n",
    "    k_means.fit(X)  # 训练 KMeans 模型\n",
    "    centers = k_means.cluster_centers_  # 获取聚类中心\n",
    "    error = sum(np.min(cdist(X, centers, 'euclidean'), axis=1))  # 计算每个点到其最近聚类中心的距离，并求和\n",
    "    mean_distortions.append(error)  # 将误差添加到列表中\n",
    "\n",
    "# 绘制肘部法则图\n",
    "plt.plot(K, mean_distortions, 'bx-')  # 绘制 K 值与平均畸变程度的关系图，使用蓝色的 'x' 标记点，并用线连接\n",
    "plt.xlabel('k')  # 设置 x 轴标签为 'k'\n",
    "plt.ylabel('平均畸变程度')  # 设置 y 轴标签为 '平均畸变程度'\n",
    "plt.title('用肘部法则来确定最佳的 K 值')  # 设置图表标题为 '用肘部法则来确定最佳的 K 值'\n",
    "# plt.savefig(\"iris1.png\", bbox_inches='tight')  # 保存图表为 'iris1.png' 文件，bbox_inches='tight' 表示紧凑布局\n",
    "plt.show()  # 显示图表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用K-means聚类分析引入新的特征，包括所属类别，距离质心的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的最佳K值从上面获得，找下降幅度最大的点，这里是3\n",
    "from sklearn.cluster import KMeans  # 从 scikit-learn 库中导入 KMeans 聚类算法\n",
    "from scipy.spatial.distance import cdist  # 从 SciPy 库中导入 cdist 函数，用于计算距离\n",
    "\n",
    "# 训练模型\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "# 读取聚类标签\n",
    "cluster_labels = kmeans.labels_\n",
    "# 计算距离质心距离\n",
    "distances_to_centroids = kmeans.transform(X)\n",
    "# 获取每个样本到其所属质心的距离\n",
    "distance_to_own_centroid = distances_to_centroids[range(len(X)), kmeans.labels_]\n",
    "\n",
    "# 将特征添加至数据集中\n",
    "X['Cluster'] = cluster_labels\n",
    "X['Distance_to_Centroid'] = distance_to_own_centroid\n",
    "\n",
    "# 打印看看\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看出标签有点不平衡，所以后面我们需要进行过采样\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离散特征和连续特征编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果需要填补缺失值，使用众数\n",
    "# for column in X.columns:\n",
    "#     if X[column].dtype in ['float64', 'int64']:\n",
    "#         mode = X[column].mode()[0]\n",
    "#         X[column].fillna(mode, inplace=True)\n",
    "\n",
    "# 分离离散特征和连续特征  这里我们把 int 和 float 都看作连续特征， 字符串看作离散特征\n",
    "continuous_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "discrete_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "# 对连续特征进行标准化处理\n",
    "scaler = StandardScaler()\n",
    "X_continuous = scaler.fit_transform(X[continuous_features])\n",
    "# 对离散特征进行独热编码  // 这里可以考虑使用标签编码减少特征的数量\n",
    "encoder = OneHotEncoder()\n",
    "X_discrete = encoder.fit_transform(X[discrete_features])\n",
    "# 合并连续特征和离散特征\n",
    "X = pd.concat([pd.DataFrame(X_continuous), pd.DataFrame(X_discrete.toarray())], axis=1)\n",
    "\n",
    "# 对标签进行数值编码\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# 使用 train_test_split 函数按照 8:2 的比例划分数据集\n",
    "# test_size=0.2 表示 20%的数据用作测试集，即验证集。\n",
    "# random_state 是一个随机数种子，确保每次划分的结果相同，便于复现结果。\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印看看数据\n",
    "# X_train.info()\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对训练集数据进行过采样， 这里我们使用SMOTE插值法产生样本 不能对考试即测试集进行过采样\n",
    "\n",
    "from imblearn.over_sampling import SMOTE # 导入 SMOTE 方法\n",
    "\n",
    "# 使用 SMOTE 进行过采样,并赋值给自己\n",
    "smote = SMOTE(random_state=42)  # 随机数种子我们还是设置为 42\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "---\n",
    "至此为止，数据初步处理已经完成，接下来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我们的基模型选择使用逻辑回归、随机森林、XGBoost、LightGBM 四种模型\n",
    "# 首先不调参，看看效果\n",
    "# 导入所需的库\n",
    "import pandas as pd  # 用于数据处理和分析\n",
    "import numpy as np  # 用于数值计算\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder  # 用于数据预处理\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # 用于数据集划分和超参数调优\n",
    "from sklearn.linear_model import LinearRegression  # 线性回归模型\n",
    "from sklearn.linear_model import LogisticRegression  # 逻辑回归模型\n",
    "from sklearn.svm import SVC  # 支持向量机分类模型\n",
    "from sklearn.naive_bayes import GaussianNB  # 高斯朴素贝叶斯模型\n",
    "from sklearn.ensemble import RandomForestClassifier  # 随机森林分类模型\n",
    "import xgboost as xgb  # XGBoost模型\n",
    "import lightgbm as lgb  # LightGBM模型\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, classification_report  # 用于模型评估\n",
    "import warnings\n",
    "# 忽略所有警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # 绘图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不调参情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归\n",
    "print('逻辑回归')\n",
    "\n",
    "lr = LogisticRegression()  # 实例化逻辑回归模型\n",
    "lr.fit(X_train, y_train)  # 训练模型\n",
    "y_pred_lr = lr.predict(X_test)  # 预测\n",
    "\n",
    "# y_pred_lr\n",
    "# 输出\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"AUC: \", roc_auc_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林\n",
    "print('随机森林')\n",
    "\n",
    "# 定义随机森林模型（使用默认参数）\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\" AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "print('SVM')\n",
    "\n",
    "# 定义 SVM 模型（使用默认参数）\n",
    "svm = SVC(probability=True, random_state=42)  # 启用概率估计\n",
    "\n",
    "# 训练模型\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = svm.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "print('XGBoost')\n",
    "\n",
    "# 定义 XGBoost 模型（使用默认参数）\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "print('LightGBM')\n",
    "\n",
    "# 定义 LightGBM 模型（使用默认参数）\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))\n",
    "# y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参，这里我们使用网格搜索（还可以使用optuna，启发式算法等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "\n",
    "# 定义逻辑回归模型\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # 正则化强度的倒数\n",
    "    'penalty': ['l1', 'l2'],       # 正则化类型\n",
    "    'solver': ['liblinear']        # 优化算法（liblinear 支持 l1 和 l2）\n",
    "}\n",
    "\n",
    "# 初始化网格搜索\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr,  # 模型\n",
    "    param_grid=param_grid,  # 参数网格\n",
    "    cv=5,  # 五折交叉验证\n",
    "    scoring='roc_auc',  # 使用 AUC 作为评估指标\n",
    "    n_jobs=-1  # 使用所有可用的CPU核心\n",
    ")\n",
    "\n",
    "# 在训练集上执行网格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"最佳参数组合:\", grid_search.best_params_)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "best_lr = grid_search.best_estimator_  # best_lr 为最佳参数模型\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_lr = best_lr.predict(X_test)\n",
    "\n",
    "# # 输出混淆矩阵\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# # 输出分类报告\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# 输出 AUC\n",
    "y_pred_proba = best_lr.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# 定义 SVM 模型\n",
    "svm = SVC(probability=True, random_state=42)  # 启用概率估计\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # 正则化参数\n",
    "    'kernel': ['linear', 'rbf'],  # 核函数\n",
    "    'gamma': ['scale', 'auto']  # 核函数的系数\n",
    "}\n",
    "\n",
    "# 初始化网格搜索\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,  # 模型\n",
    "    param_grid=param_grid,  # 参数网格\n",
    "    cv=5,  # 五折交叉验证\n",
    "    scoring='roc_auc',  # 使用 AUC 作为评估指标\n",
    "    n_jobs=-1  # 使用所有可用的CPU核心\n",
    ")\n",
    "\n",
    "# 在训练集上执行网格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"最佳参数组合:\", grid_search.best_params_)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_svm = best_svm.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林\n",
    "# 定义随机森林模型\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # 树的数量\n",
    "    'max_depth': [None, 10, 20],     # 每棵树的最大深度\n",
    "    'min_samples_split': [2, 5, 10], # 分裂内部节点所需的最小样本数\n",
    "    'min_samples_leaf': [1, 2, 4],   # 叶子节点所需的最小样本数\n",
    "    'max_features': ['sqrt', 'log2'] # 每棵树分裂时考虑的最大特征数\n",
    "}\n",
    "\n",
    "# 初始化网格搜索\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,  # 模型\n",
    "    param_grid=param_grid,  # 参数网格\n",
    "    cv=5,  # 五折交叉验证\n",
    "    scoring='roc_auc',  # 使用 AUC 作为评估指标\n",
    "    n_jobs=-1  # 使用所有可用的CPU核心\n",
    ")\n",
    "\n",
    "# 在训练集上执行网格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"最佳参数组合:\", grid_search.best_params_)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# 定义 XGBoost 模型\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'max_depth': [3, 6, 9],  # 树的最大深度\n",
    "    'learning_rate': [0.01, 0.1, 0.3],  # 学习率\n",
    "    'n_estimators': [100, 200, 300],  # 树的数量\n",
    "    'subsample': [0.8, 1.0],  # 样本采样比例\n",
    "    'colsample_bytree': [0.8, 1.0]  # 特征采样比例\n",
    "}\n",
    "\n",
    "# 初始化网格搜索\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,  # 模型\n",
    "    param_grid=param_grid,  # 参数网格\n",
    "    cv=5,  # 五折交叉验证\n",
    "    scoring='roc_auc',  # 使用 AUC 作为评估指标\n",
    "    n_jobs=-1  # 使用所有可用的CPU核心\n",
    ")\n",
    "\n",
    "# 在训练集上执行网格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"最佳参数组合:\", grid_search.best_params_)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "# 定义 LightGBM 模型\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 63, 127],  # 每棵树的最大叶子节点数\n",
    "    'learning_rate': [0.01, 0.1, 0.3],  # 学习率\n",
    "    'n_estimators': [100, 200, 300],  # 树的数量\n",
    "    'max_depth': [5, 10, -1],  # 树的最大深度，-1 表示不限制\n",
    "    'subsample': [0.8, 1.0],  # 样本采样比例\n",
    "    'colsample_bytree': [0.8, 1.0]  # 特征采样比例\n",
    "}\n",
    "\n",
    "# 初始化网格搜索\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb_model,  # 模型\n",
    "    param_grid=param_grid,  # 参数网格\n",
    "    cv=5,  # 五折交叉验证\n",
    "    scoring='roc_auc',  # 使用 AUC 作为评估指标\n",
    "    n_jobs=-1  # 使用所有可用的CPU核心\n",
    ")\n",
    "\n",
    "# 在训练集上执行网格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"最佳参数组合:\", grid_search.best_params_)\n",
    "\n",
    "# 使用最佳参数训练模型\n",
    "best_lgb = grid_search.best_estimator_\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred_proba = best_lgb.predict_proba(X_test)[:, 1]  # 获取正类的概率\n",
    "\n",
    "# 输出测试集 AUC\n",
    "print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合（这里直接使用调参后的模型，不考虑未条参的模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking进行模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里使用逻辑回归、随机森林、XGBoost作为基模型，LightGBM做元模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier  # 导入 Stacking 分类器\n",
    "# from sklearn.linear_model import LogisticRegression  # 逻辑回归模型\n",
    "# from sklearn.ensemble import RandomForestClassifier  # 随机森林分类模型\n",
    "# import xgboost as xgb  # XGBoost模型\n",
    "# import lightgbm as lgb  # LightGBM模型\n",
    "# # 定义基模型（使用调参后的最优参数）\n",
    "# base_models = [\n",
    "#     ('rf', RandomForestClassifier(\n",
    "#         max_depth=20,  # 最优参数\n",
    "#         max_features=0.8,  # 最优参数\n",
    "#         min_samples_leaf=1,  # 最优参数\n",
    "#         min_samples_split=2,  # 最优参数\n",
    "#         n_estimators=120,  # 最优参数\n",
    "#         random_state=42\n",
    "#     )),  # 随机森林\n",
    "#     ('xgb', XGBClassifier(\n",
    "#         colsample_bytree=1.0,  # 最优参数\n",
    "#         learning_rate=0.1,  # 最优参数\n",
    "#         max_depth=9,  # 最优参数\n",
    "#         n_estimators=150,  # 最优参数\n",
    "#         reg_alpha=0,  # 最优参数\n",
    "#         reg_lambda=1,  # 最优参数\n",
    "#         subsample=0.8,  # 最优参数\n",
    "#         random_state=42\n",
    "#     )),  # XGBoost\n",
    "#     ('lr', LogisticRegression(\n",
    "#         colsample_bytree=1.0,  # 最优参数\n",
    "#         learning_rate=0.1,  # 最优参数\n",
    "#         num_leaves=100,  # 最优参数\n",
    "#         subsample=0.8,  # 最优参数\n",
    "#         random_state=42\n",
    "#     ))  # 逻辑回归\n",
    "# ]\n",
    "# # 定义元模型（LightGBM）\n",
    "# meta_model_lgbm = LGBMClassifier(\n",
    "#     colsample_bytree=0.9,  # 微调\n",
    "#     learning_rate=0.05,  # 降低学习率\n",
    "#     num_leaves=50,  # 微调\n",
    "#     n_estimators=200,  # 增加树的数量\n",
    "#     reg_alpha=0.1,  # 微调 L1 正则化\n",
    "#     reg_lambda=0.5,  # 微调 L2 正则化\n",
    "#     subsample=0.9,  # 微调\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # 创建 Stacking 回归器\n",
    "# stacking_model_lgbm = StackingClassifier(\n",
    "#     estimators=base_models,  # 基模型\n",
    "#     final_estimator=meta_model_lgbm,  # LightGBM 作为元模型\n",
    "#     n_jobs=1  # 使用1 CPU 核心\n",
    "# )\n",
    "\n",
    "# # 训练 Stacking 模型\n",
    "# stacking_model_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# # 预测\n",
    "# y_test_pred_lgbm = stacking_model_lgbm.predict(X_test)[:, 1]\n",
    "\n",
    "# # 计算 AUC\n",
    "\n",
    "# print(\"测试集 AUC: \", roc_auc_score(y_test, y_test_pred_lgbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加权进行融合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义基模型（使用你提供的参数）\n",
    "# base_models = [\n",
    "#     ('rf', RandomForestClassifier(\n",
    "#         max_depth=20,  # 最优参数\n",
    "#         max_features=0.8,  # 最优参数\n",
    "#         min_samples_leaf=1,  # 最优参数\n",
    "#         min_samples_split=2,  # 最优参数\n",
    "#         n_estimators=120,  # 最优参数\n",
    "#         random_state=42\n",
    "#     )),  # 随机森林\n",
    "#     ('xgb', XGBClassifier(\n",
    "#         colsample_bytree=1.0,  # 最优参数\n",
    "#         learning_rate=0.1,  # 最优参数\n",
    "#         max_depth=9,  # 最优参数\n",
    "#         n_estimators=150,  # 最优参数\n",
    "#         reg_alpha=0,  # 最优参数\n",
    "#         reg_lambda=1,  # 最优参数\n",
    "#         subsample=0.8,  # 最优参数\n",
    "#         random_state=42\n",
    "#     )),  # XGBoost\n",
    "#     ('lgbm', LGBMClassifier(\n",
    "#         colsample_bytree=1.0,  # 最优参数\n",
    "#         learning_rate=0.1,  # 最优参数\n",
    "#         num_leaves=100,  # 最优参数\n",
    "#         subsample=0.8,  # 最优参数\n",
    "#         random_state=42\n",
    "#     ))  # LightGBM\n",
    "# ]\n",
    "\n",
    "# # 训练基模型\n",
    "# for name, model in base_models:\n",
    "#     model.fit(X_train, y_train)\n",
    "#     print(f\"{name} 模型训练完成\")\n",
    "\n",
    "# # 获取每个模型的预测结果\n",
    "# y_test_pred_rf = base_models[0][1].predict(X_test)  # 随机森林\n",
    "# y_test_pred_xgb = base_models[1][1].predict(X_test)  # XGBoost\n",
    "# y_test_pred_lgbm = base_models[2][1].predict(X_test)  # LightGBM\n",
    "\n",
    "# # 定义网格搜索范围\n",
    "# weights_rf = np.linspace(0, 1, 11)  # 随机森林权重范围 [0, 0.1, ..., 1]\n",
    "# weights_xgb = np.linspace(0, 1, 11)  # XGBoost 权重范围 [0, 0.1, ..., 1]\n",
    "# weights_lgbm = np.linspace(0, 1, 11)  # LightGBM 权重范围 [0, 0.1, ..., 1]\n",
    "\n",
    "# # 初始化最佳 AUC 和最佳权重\n",
    "# best_rmse = float('inf')\n",
    "# best_weights = None\n",
    "\n",
    "# # 网格搜索\n",
    "# for w_rf in weights_rf:\n",
    "#     for w_xgb in weights_xgb:\n",
    "#         for w_lgbm in weights_lgbm:\n",
    "#             if w_rf + w_xgb + w_lgbm == 1:  # 确保权重之和为 1\n",
    "#                 # 加权平均\n",
    "#                 y_test_pred_weighted = (w_rf * y_test_pred_rf +\n",
    "#                                         w_xgb * y_test_pred_xgb +\n",
    "#                                         w_lgbm * y_test_pred_lgbm)\n",
    "#                 # 计算 RMSE\n",
    "#                 test_rmse = mean_squared_error(y_test, y_test_pred_weighted, squared=False)\n",
    "#                 # 更新最佳权重\n",
    "#                 if test_rmse < best_rmse:\n",
    "#                     best_rmse = test_rmse\n",
    "#                     best_weights = (w_rf, w_xgb, w_lgbm)\n",
    "\n",
    "# # 输出最佳权重和 RMSE\n",
    "# print(f\"最佳权重：随机森林={best_weights[0]:.2f}, XGBoost={best_weights[1]:.2f}, LightGBM={best_weights[2]:.2f}\")\n",
    "# print(f\"最佳 AUC: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用阈值进行二次调参 thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# model = stacking_model_lgbm\n",
    "\n",
    "# # 在测试集上进行预测\n",
    "# y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# # 输出测试集 AUC\n",
    "# print(\"测试集 AUC: \", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# # 初始化列表用于存储不同阈值下的准确率 --- 这里使用准确率作为评价指标\n",
    "# accuracies = []\n",
    "# thresholds = np.arange(0.1, 1, 0.01)\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     accuracies.append(accuracy)\n",
    "\n",
    "# # 找到最大准确率及其对应的阈值\n",
    "# max_accuracy_index = np.argmax(accuracies)\n",
    "# optimal_threshold = thresholds[max_accuracy_index]\n",
    "# max_accuracy = accuracies[max_accuracy_index]\n",
    "\n",
    "# print(f\"最佳阈值: {optimal_threshold}\")\n",
    "# print(f\"最大准确率: {max_accuracy}\")\n",
    "\n",
    "# # 绘制阈值 - 准确率曲线\n",
    "# plt.plot(thresholds, accuracies)\n",
    "# plt.xlabel('Threshold')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Threshold - Accuracy Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可解释分析\n",
    "\n",
    "---\n",
    "\n",
    "使用Shap库进行可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump,load\n",
    "\n",
    "# # 保存模型\n",
    "# dump(model, 'model.joblib')\n",
    "\n",
    "# # 加载模型\n",
    "# model = load('model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
